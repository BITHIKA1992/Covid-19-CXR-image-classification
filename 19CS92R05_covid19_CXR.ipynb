{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lq9qw48Brnjs",
    "outputId": "b87e0316-6b8d-45c5-cf70-d2e06b7ab00b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "CNjusHdlUHoZ",
    "outputId": "c74f91fa-f76d-4b88-8bc2-f63b7f8fc1f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-ignite in /usr/local/lib/python3.6/dist-packages (0.3.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.5.0+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-ignite) (1.18.4)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-ignite) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-ignite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task:\n",
    "### Covid-19 XRay-Image classification\n",
    "### Dataset: http://ieee-dataport.org/open-access/covid19action-radiology-cxr\n",
    "### Class: Covid-19-positive, Pneumonia, Non-Pneumonia\n",
    "### Model: Developed on LeNet\n",
    "\n",
    "Note: Dataset usage guide - citation for the data Source-4 is corrected as https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "csAh_Nvlr082"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from ignite.handlers import Timer\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from torchvision import models\n",
    "import io\n",
    "from torch.utils.data import Dataset\n",
    "import zipfile\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PqXFfn-0ugE0"
   },
   "outputs": [],
   "source": [
    "class RadiologyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, txt_path='filelist.txt', img_dir='data/', transform=None, train=True, val=False, val_partition=5):\n",
    "        \"\"\"\n",
    "        Initialize data set as a list of IDs corresponding to each item of data set\n",
    "\n",
    "        :param img_dir: path to image files as a uncompressed tar archive\n",
    "        :param txt_path: a text file containing names of all of images line by line\n",
    "        :param transform: apply some transforms like cropping, rotating, etc on input image\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(txt_path, encoding='utf-8', engine='python')\n",
    "        #df['class'] = df['Non-Pneumonia'].astype(str)+'_'+df['Other Pneumonia'].astype(str)+'_'+df['COVID-19'].astype(str)\n",
    "        #df['class'].replace({\"0_0_1\": 0, \"0_1_0\": 1, \"1_0_0\": 2, \"-1_-1_0\": 3}, inplace=True)\n",
    "        if train == True:\n",
    "            df = df[df.Partition != val_partition].reset_index().drop(['index'], axis=1)\n",
    "        elif val == True:\n",
    "            df = df[df.Partition == val_partition].reset_index().drop(['index'], axis=1)\n",
    "        else:\n",
    "            df = df\n",
    "        df['class'] = df['Non-Pneumonia'].astype(str) + '_' + df['Other Pneumonia'].astype(str) + '_' + df['COVID-19'].astype(str)\n",
    "        df = df[df['class'] != '-1_-1_0'].reset_index().drop(['index'], axis=1)\n",
    "        df['class'].replace({\"0_0_1\": 0, \"0_1_0\": 1, \"1_0_0\": 2}, inplace=True)\n",
    "        self.df = df\n",
    "        self.img_names = df.index.values\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "        self.source3_image_selector = False\n",
    "        self.source4_image_selector = False\n",
    "        self.source3_zip_handle = None\n",
    "        self.source4_zip_handle = None\n",
    "\n",
    "    def get_image_from_zip(self, source, zip_name, zip_dir, name):\n",
    "        \"\"\"\n",
    "        Gets a image by a name gathered from file list csv file\n",
    "\n",
    "        :param name: name of targeted image\n",
    "        :return: a PIL image\n",
    "        \"\"\"\n",
    "        if source == 'Source-3':\n",
    "            if self.source3_image_selector == False:\n",
    "                self.source3_zip_handle = zipfile.ZipFile(self.img_dir + zip_name, 'r')\n",
    "                self.source3_image_selector = True\n",
    "            archive = self.source3_zip_handle\n",
    "        elif source == 'Source-4':\n",
    "            if self.source4_image_selector == False:\n",
    "                self.source4_zip_handle = zipfile.ZipFile(self.img_dir + zip_name, 'r')\n",
    "                self.source4_image_selector = True\n",
    "            archive = self.source4_zip_handle\n",
    "        else:\n",
    "            archive = zipfile.ZipFile(self.img_dir + zip_name, 'r')\n",
    "\n",
    "        image = archive.read(zip_dir + name)\n",
    "        image = Image.open(io.BytesIO(image))\n",
    "        #archive.close()\n",
    "        \n",
    "        return image\n",
    "\n",
    "    def get_image_from_folder(self, name):\n",
    "        \"\"\"\n",
    "        gets a image by a name gathered from file list text file\n",
    "\n",
    "        :param name: name of targeted image\n",
    "        :return: a PIL image\n",
    "        \"\"\"\n",
    "\n",
    "        image = Image.open(self.img_dir + name)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of data set using list of IDs\n",
    "\n",
    "        :return: number of samples in data set\n",
    "        \"\"\"\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generate one item of data set.\n",
    "\n",
    "        :param index: index of item in IDs list\n",
    "\n",
    "        :return: a sample of data as a dict\n",
    "        \"\"\"\n",
    "        source = self.df.iloc[index]['Data Source']\n",
    "        image_name = self.df.iloc[index]['Image Name']\n",
    "\n",
    "        if source == 'Source-3':\n",
    "            zip_name = source + '/CheXpert-v1.0-small.zip'\n",
    "            temp = image_name.split('/')[1].split('__')\n",
    "            temp[0] = temp[0]+'-small'\n",
    "            zip_dir = '/'.join(  temp[:-1]) + '/'\n",
    "            image_name = temp[-1]\n",
    "            X = self.get_image_from_zip(source, zip_name, zip_dir, image_name)\n",
    "        \n",
    "        elif source == 'Source-4':\n",
    "            if self.train == True or self.val == True:\n",
    "                zip_dir = 'chest-xray-pneumonia/chest_xray/train/'\n",
    "            else:\n",
    "                zip_dir = 'chest-xray-pneumonia/chest_xray/test/'\n",
    "            zip_name = source + '/chest-xray-pneumonia.zip'\n",
    "            X = self.get_image_from_zip(source, zip_name, zip_dir, image_name)\n",
    "\n",
    "        elif source == 'Source-6':\n",
    "            try:\n",
    "                X = self.get_image_from_folder(source + '/' + image_name+'.jpg')\n",
    "            except FileNotFoundError:\n",
    "                X = self.get_image_from_folder(source + '/' + image_name+'.png')\n",
    "          \n",
    "          \n",
    "        else:\n",
    "            X = self.get_image_from_folder(source + '/' + image_name)\n",
    "        \n",
    "        flag = False\n",
    "        #if X.mode == 'L':\n",
    "        #  flag = True\n",
    "        #if X.mode == 'RGBA':\n",
    "        #  X = X.convert('RGB')\n",
    "        X = X.convert('L')\n",
    "\n",
    "\n",
    "        # Get you label here using available pandas functions\n",
    "        y = self.df.iloc[index]['class']\n",
    "\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "            if flag:\n",
    "                X = transforms.Compose([transforms.Lambda(lambda x: x.repeat(3, 1, 1) )])(X)\n",
    "\n",
    "        if index == (self.__len__() - 1):\n",
    "            if self.source3_image_selector:  # close tarfile opened in __init__\n",
    "                self.source3_image_selector = False\n",
    "                self.source3_zip_handle.close()                \n",
    "            if self.source4_image_selector:  # close tarfile opened in __init__\n",
    "                self.source4_image_selector = False\n",
    "                self.source4_zip_handle.close()\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "63d0CoV_RX7N",
    "outputId": "f8183d78-a3d7-404c-f8c2-37f7d46b595e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is avaialble!\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 32,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 8}\n",
    "\n",
    "vparams = {'batch_size': 32,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 8}\n",
    "\n",
    "max_epochs = 100\n",
    "n_splits = 5\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([transforms.Resize(size = (256, 256)),\\\n",
    "                                      transforms.RandomRotation(10, resample=Image.BILINEAR), \\\n",
    "                                      transforms.ToTensor(), \\\n",
    "                                      transforms.Normalize((0.45,), (0.22,)) ])\n",
    "test_transform = transforms.Compose([ transforms.Resize(size = (256, 256)),\\\n",
    "                                      transforms.ToTensor(), \\\n",
    "                                      transforms.Normalize((0.45,), (0.22,)) ])\n",
    "\n",
    "# Generators\n",
    "txt_path = '/content/drive/My Drive/Covid-19-chest-xray-classification/'\n",
    "img_dir =  '/content/drive/My Drive/Covid-19-chest-xray-classification/images/'\n",
    "\n",
    "training_set = RadiologyDataset(txt_path + 'Train_Combined.csv', img_dir, transform = train_transform, train=True, val=False, val_partition=5)\n",
    "trainLoader = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = RadiologyDataset(txt_path + 'Train_Combined.csv', img_dir, transform = test_transform, train=False, val=True, val_partition=5)\n",
    "validationLoader = data.DataLoader(validation_set, **vparams)\n",
    "\n",
    "test_set = RadiologyDataset(txt_path + 'Test_Combined.csv', img_dir, transform = test_transform, train=False, val=False, val_partition=0)\n",
    "testLoader = data.DataLoader(test_set, **vparams)\n",
    "\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print('GPU is avaialble!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QJv43JM42FgW"
   },
   "outputs": [],
   "source": [
    "print(len(training_set), len(validation_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7C_yOMfdnYV"
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=3, stride= 1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=3, stride= 1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2,stride=2) \n",
    "        self.conv3 = nn.Conv2d(6, 32, kernel_size=3, stride= 1, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2,stride=2) \n",
    "        self.conv4 = nn.Conv2d(6, 48, kernel_size=3, stride= 1, padding=1)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2,stride=2)   \n",
    "        self.conv5 = nn.Conv2d(6, 64, kernel_size=3, stride= 1, padding=1)\n",
    "        self.pool5 = nn.MaxPool2d(kernel_size=4,stride=4)      \n",
    "        self.fc1 = nn.Linear(1024, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool3(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool4(x)\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool5(x)\n",
    "        x = x.view(-1, 1024)\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fqW-tgSnPgVw"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    net = LeNet()\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fu6A4jTNbJ8C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142,279 total parameters.\n",
      "142,279 training parameters.\n"
     ]
    }
   ],
   "source": [
    "net = get_model()\n",
    "\n",
    "# Find total parameters and trainable parameters\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_J4OldfwQ17h"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, save_dir = './' , fold_ = ''):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_acc_max = -np.Inf\n",
    "        self.train_acc_max = -np.Inf\n",
    "        self.delta = delta\n",
    "        self.fold_ = fold_\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "    def __call__(self, val_acc, train_acc, model):\n",
    "\n",
    "        score = val_acc\n",
    "        tscore = train_acc\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_tscore = tscore\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "        elif score <= self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "            if tscore > self.best_tscore:\n",
    "                self.best_tscore = tscore\n",
    "                self.save_tcheckpoint(train_acc, model)\n",
    "                self.counter = 0\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_acc, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_acc, model):\n",
    "        '''Saves model when validation accuracy improves.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation accuracy increased ({self.val_acc_max:.6f} --> {val_acc:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.save_dir+'lenet_checkpoint'+str(self.fold_)+'.pt')\n",
    "        self.val_acc_max = val_acc\n",
    "\n",
    "    def save_tcheckpoint(self, train_acc, model):\n",
    "        '''Saves model when train accuracy improves.'''\n",
    "        if self.verbose:\n",
    "            print(f'Train accuracy increased ({self.train_acc_max:.6f} --> {train_acc:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.save_dir+'lenet_checkpoint'+str(self.fold_)+'.pt')\n",
    "        self.train_acc_max = train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gANgaNvIUk3Y"
   },
   "outputs": [],
   "source": [
    "def test_accuracy_calculate():\n",
    "    correct_pred = 0\n",
    "    t.reset()\n",
    "\n",
    "    actual_labels = []\n",
    "    pred_lables = []\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for data_ in testLoader:\n",
    "            inputs,labels = data_\n",
    "            if use_gpu:\n",
    "                inputs, labels = inputs.cuda(),labels.cuda()\n",
    "            t.resume()\n",
    "            # Feedforward train data batch through model\n",
    "            output = net(inputs) \n",
    "            # Predicted class is the one with maximum probability\n",
    "            preds = torch.argmax(output,dim=1)\n",
    "            correct_pred += torch.sum(preds==labels)\n",
    "            t.pause()\n",
    "            t.step()\n",
    "\n",
    "            actual_labels = actual_labels + list(labels.cpu().detach().numpy())\n",
    "            pred_lables = pred_lables + list(preds.cpu().detach().numpy())\n",
    "\n",
    "    test_accuracy = correct_pred.item()/len(test_set)\n",
    "    print('Testing accuracy = ',test_accuracy*100, 'Computational Time = ', t.value())\n",
    "    return test_accuracy * 100, actual_labels, pred_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R4cQUgxmHehx"
   },
   "outputs": [],
   "source": [
    "### Set the Timer to compute the inference \n",
    "t = Timer(average=True)\n",
    "t.reset()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLmJlMw8HhM4"
   },
   "outputs": [],
   "source": [
    "kfold_train_acc = []\n",
    "kfold_validation_acc = []\n",
    "kfold_test_acc = []\n",
    "\n",
    "kfold_train_loss = []\n",
    "kfold_validation_loss = []\n",
    "n_splits =1\n",
    "for sp_ in range(n_splits):\n",
    "    fold_ = sp_ + 1\n",
    "    print(\"*\"*100)\n",
    "    print(\"Training for cross-fold number:\", fold_)\n",
    "    print(\"*\"*100)\n",
    "    ##############  Initialize the Network  ###########\n",
    "\n",
    "    #net = get_model()\n",
    "    net = LeNet()\n",
    "    if use_gpu:\n",
    "        net = net.cuda()\n",
    "        net = nn.DataParallel(net)\n",
    "\n",
    "    ##############  Define the Loss function  ##########\n",
    "    criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "    ##############  Define the Optimizer  #################\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.5)\n",
    "    #optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "    ##############  Define the scheduler  (if require warm restart to set the learning rate)   #################\n",
    "    #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(trainLoader), epochs=num_epochs, anneal_strategy='linear') #steps_per_epoch=len(trainLoader), epochs=num_epochs,\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    ##############  Set the Loader -- For current k-fold  ###########\n",
    "    training_set = RadiologyDataset(txt_path + 'Train_Combined.csv', img_dir, transform = train_transform, train=True, val=False, val_partition=fold_)\n",
    "    trainLoader = data.DataLoader(training_set, **params)\n",
    "\n",
    "    validation_set = RadiologyDataset(txt_path + 'Train_Combined.csv', img_dir, transform = test_transform, train=False, val=True, val_partition=fold_)\n",
    "    validationLoader = data.DataLoader(validation_set, **vparams)\n",
    "\n",
    "\n",
    "    #############   Declare the place holder for storing -- loss and accuracy for each epoch   ########\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    validation_loss =[]\n",
    "    validation_acc = []\n",
    "\n",
    "    #############  Initialize the Early-Stopping criterea of training   #########\n",
    "    early_stopping = EarlyStopping(patience=20, verbose=True, delta=0, save_dir = img_dir , fold_ = str(fold_))\n",
    "\n",
    "    num_epochs = 20\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        t.reset()  ## --- restart timer\n",
    "\n",
    "        #####################################\n",
    "        #######          Train      #########   \n",
    "        ##################################### \n",
    "        running_loss = 0.0 \n",
    "        running_corr = 0\n",
    "        for i,data_ in tqdm(enumerate(trainLoader)):\n",
    "            #print(fold_, epoch, i)\n",
    "            t.resume()   # ---- timer on\n",
    "            inputs,labels = data_\n",
    "            if use_gpu:\n",
    "                inputs, labels = inputs.cuda(),labels.cuda() \n",
    "            # Initializing model gradients to zero\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            \n",
    "            # Data feed-forward through the network\n",
    "            outputs = net(inputs)\n",
    "            # Predicted class is the one with maximum probability\n",
    "            preds = torch.argmax(outputs,dim=1)\n",
    "            # Finding the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            #scheduler.step()\n",
    "            t.pause()   # ---- timer off\n",
    "\n",
    "            # Accumulating the loss for each batch\n",
    "            running_loss += loss \n",
    "            # Accumulate number of correct predictions\n",
    "            running_corr += torch.sum(preds==labels)    \n",
    "            \n",
    "\n",
    "        epoch_loss = running_loss.item()/(i+1)   #Total loss for one epoch\n",
    "        epoch_acc = running_corr.item()/len(training_set)\n",
    "\n",
    "        train_loss.append(epoch_loss) #Saving the loss over epochs for plotting the graph\n",
    "        train_acc.append(epoch_acc) #Saving the accuracy over epochs for plotting the graph\n",
    "\n",
    "        #####################################\n",
    "        #######     Validation      #########   \n",
    "        ##################################### \n",
    "\n",
    "        val_running_loss = 0.0 \n",
    "        val_running_corr = 0\n",
    "        ### Validation \n",
    "        with torch.no_grad():   #torch.set_grad_enabled(False):\n",
    "            for j, data_ in tqdm(enumerate(validationLoader)):\n",
    "                inputs,labels = data_\n",
    "                if use_gpu:\n",
    "                    inputs, labels = inputs.cuda(),labels.cuda() \n",
    "                t.resume()   # ---- timer on\n",
    "                outputs = net(inputs)\n",
    "                preds = torch.argmax(outputs,dim=1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                t.pause()   # ---- timer off\n",
    "                val_running_loss += loss \n",
    "                val_running_corr += torch.sum(preds==labels) \n",
    "\n",
    "        val_epoch_loss = val_running_loss.item()/(j+1)   #Total loss for one epoch\n",
    "        val_epoch_acc = val_running_corr.item()/len(validation_set)\n",
    "\n",
    "        validation_loss.append(val_epoch_loss) #Saving the loss over epochs for plotting the graph\n",
    "        validation_acc.append(val_epoch_acc) #Saving the accuracy over epochs for plotting the graph    \n",
    "\n",
    "        #########   Scheduler to update the learning rate   #######\n",
    "        scheduler.step(val_epoch_loss)    \n",
    "        \n",
    "        #########   Check for early-stopping based on validation accuracy and training accuracy   ###########\n",
    "        early_stopping(val_epoch_acc, epoch_acc, net)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "            \n",
    "        t.step()   #-- timer average\n",
    "\n",
    "        #if (epoch) % 5 == 0:\n",
    "        print('Epoch {:.0f}/{:.0f} : Training loss: {:.4f} | Training Accuracy: {:.4f} : Validation loss: {:.4f} | Validation Accuracy: {:.4f} | Computational Time: {:.4f} sec'\\\n",
    "              .format(epoch+1, num_epochs, epoch_loss, epoch_acc*100, val_epoch_loss, val_epoch_acc*100, t.value()))\n",
    "        \n",
    "    # load the last checkpoint with the best model\n",
    "    net.load_state_dict(torch.load(img_dir+'lenet_checkpoint'+str(fold_)+'.pt'))\n",
    "\n",
    "\n",
    "    ############  Calculate Test Accuracy  #######\n",
    "    test_acc, test_labels, pred_labels= test_accuracy_calculate()\n",
    "\n",
    "\n",
    "    kfold_train_acc.append(train_acc)\n",
    "    kfold_validation_acc.append(validation_acc)\n",
    "    kfold_test_acc.append(test_acc)\n",
    "\n",
    "    kfold_train_loss.append(train_loss)\n",
    "    kfold_validation_loss.append(validation_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aqMI-L-n2wX6"
   },
   "outputs": [],
   "source": [
    "#test_acc, test_labels, preds_labels = test_accuracy_calculate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4L5COxHeRZBn"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,  multilabel_confusion_matrix\n",
    "\n",
    "acc = accuracy_score(test_labels, np.round(preds_labels))*100\n",
    "cm = multilabel_confusion_matrix(test_labels, np.round(preds_labels))\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"#\"*20)\n",
    "    if i == 0:\n",
    "        print(\"Covid Vs Non-Covid\")\n",
    "    elif i == 1:\n",
    "        print(\"Pneumonia Vs Non-Pneumonia\")\n",
    "    else:\n",
    "        print(\"Normal Vs Non-Normal\")\n",
    "    print(\"#\"*20)\n",
    "    tn, fp, fn, tp = cm[i].ravel()\n",
    "    print('CONFUSION MATRIX --------')\n",
    "    print(cm[i])\n",
    "\n",
    "    print('\\nTEST METRICS ------------')\n",
    "    precision = tp/(tp+fp)*100\n",
    "    recall = tp/(tp+fn)*100\n",
    "    print('Accuracy: {}%'.format(acc))\n",
    "    print('Precision: {}%'.format(precision))\n",
    "    print('Recall: {}%'.format(recall))\n",
    "    print('F1-score: {}'.format(2*precision*recall/(precision+recall)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SzxXFZGBIkbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_summary\n",
      "\n",
      "Layer_name\t\t\t\t\t\t\tNumber of Parameters\n",
      "====================================================================================================\n",
      "\n",
      "Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\t\t\t60\n",
      "\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\t\t\t0\n",
      "\n",
      "Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\t\t\t880\n",
      "\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\t\t\t0\n",
      "\n",
      "Conv2d(6, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\t\t\t1760\n",
      "\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\t\t\t0\n",
      "\n",
      "Conv2d(6, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\t\t\t2640\n",
      "\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\t\t\t0\n",
      "\n",
      "Conv2d(6, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\t\t\t3520\n",
      "\n",
      "MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\t\t\t0\n",
      "\n",
      "Linear(in_features=1024, out_features=120, bias=True)\t\t\t123000\n",
      "\n",
      "Linear(in_features=120, out_features=84, bias=True)\t\t\t10164\n",
      "\n",
      "Linear(in_features=84, out_features=3, bias=True)\t\t\t255\n",
      "====================================================================================================\n",
      "Total Params:142279\n"
     ]
    }
   ],
   "source": [
    "def model_summary(model):\n",
    "    print(\"model_summary\")\n",
    "    print()\n",
    "    print(\"Layer_name\"+\"\\t\"*7+\"Number of Parameters\")\n",
    "    print(\"=\"*100)\n",
    "    model_parameters = [layer for layer in model.parameters() if layer.requires_grad]\n",
    "    layer_name = [child for child in model.children()]\n",
    "    j = 0\n",
    "    total_params = 0\n",
    "    for i in layer_name:\n",
    "        print()\n",
    "        param = 0\n",
    "        try:\n",
    "            if i.bias is not None:\n",
    "                if len(i.bias) == 0:\n",
    "                    param =model_parameters[j].numel()\n",
    "                    j = j+1\n",
    "                else:\n",
    "                    param =model_parameters[j].numel()+model_parameters[j+1].numel()\n",
    "                    j = j+2\n",
    "\n",
    "        except:\n",
    "            bias = False  \n",
    "            param = 0\n",
    "\n",
    "        print(str(i)+\"\\t\"*3+str(param))\n",
    "        total_params+=param\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Total Params:{total_params}\")       \n",
    "\n",
    "model_summary(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2dr54-1vZ0XW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
