{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "ZNwhtqF709zb",
    "outputId": "32236f9d-ee3e-4320-ae6f-0c9712ad3104"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "IeyJLOMC2sh9",
    "outputId": "5f44eee8-8a2f-4c8b-d219-2371e8938116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch-ignite\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/55/41e8a995876fd2ade29bdba0c3efefa38e7d605cb353c70f3173c04928b5/pytorch_ignite-0.3.0-py2.py3-none-any.whl (103kB)\n",
      "\r",
      "\u001b[K     |███▏                            | 10kB 19.7MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 20kB 1.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 30kB 2.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 40kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 51kB 2.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 61kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 71kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 81kB 2.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 92kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 102kB 2.8MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 112kB 2.8MB/s \n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.5.0+cu101)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-ignite) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->pytorch-ignite) (1.18.4)\n",
      "Installing collected packages: pytorch-ignite\n",
      "Successfully installed pytorch-ignite-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-ignite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task:\n",
    "### Covid-19 XRay-Image classification\n",
    "### Dataset: http://ieee-dataport.org/open-access/covid19action-radiology-cxr\n",
    "### Class: Covid-19-positive, Pneumonia, Non-Pneumonia\n",
    "### Model: Developed on AlexNet\n",
    "\n",
    "Note: Dataset usage guide - citation for the data Source-4 is corrected as https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLPwu3tg28Ps"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms, datasets\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from ignite.handlers import Timer\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from torchvision import models\n",
    "import io\n",
    "from torch.utils.data import Dataset\n",
    "import zipfile\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cRb8iHnI3Cel"
   },
   "outputs": [],
   "source": [
    "class RadiologyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, txt_path='filelist.txt', img_dir='data/', transform=None, train=True, val=False, val_partition=5):\n",
    "        \"\"\"\n",
    "        Initialize data set as a list of IDs corresponding to each item of data set\n",
    "\n",
    "        :param img_dir: path to image files as a uncompressed tar archive\n",
    "        :param txt_path: a text file containing names of all of images line by line\n",
    "        :param transform: apply some transforms like cropping, rotating, etc on input image\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(txt_path, encoding='utf-8', engine='python')\n",
    "        #df['class'] = df['Non-Pneumonia'].astype(str)+'_'+df['Other Pneumonia'].astype(str)+'_'+df['COVID-19'].astype(str)\n",
    "        #df['class'].replace({\"0_0_1\": 0, \"0_1_0\": 1, \"1_0_0\": 2, \"-1_-1_0\": 3}, inplace=True)\n",
    "        if train == True:\n",
    "            df = df[df.Partition != val_partition].reset_index().drop(['index'], axis=1)\n",
    "        elif val == True:\n",
    "            df = df[df.Partition == val_partition].reset_index().drop(['index'], axis=1)\n",
    "        else:\n",
    "            df = df\n",
    "        df['class'] = df['Non-Pneumonia'].astype(str) + '_' + df['Other Pneumonia'].astype(str) + '_' + df['COVID-19'].astype(str)\n",
    "        df = df[df['class'] != '-1_-1_0'].reset_index().drop(['index'], axis=1)\n",
    "        df['class'].replace({\"0_0_1\": 0, \"0_1_0\": 1, \"1_0_0\": 2}, inplace=True)\n",
    "        self.df = df\n",
    "        self.img_names = df.index.values\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "        self.source3_image_selector = False\n",
    "        self.source4_image_selector = False\n",
    "        self.source3_zip_handle = None\n",
    "        self.source4_zip_handle = None\n",
    "\n",
    "    def get_image_from_zip(self, source, zip_name, zip_dir, name):\n",
    "        \"\"\"\n",
    "        Gets a image by a name gathered from file list csv file\n",
    "\n",
    "        :param name: name of targeted image\n",
    "        :return: a PIL image\n",
    "        \"\"\"\n",
    "        if source == 'Source-3':\n",
    "            if self.source3_image_selector == False:\n",
    "                self.source3_zip_handle = zipfile.ZipFile(self.img_dir + zip_name, 'r')\n",
    "                self.source3_image_selector = True\n",
    "            archive = self.source3_zip_handle\n",
    "        elif source == 'Source-4':\n",
    "            if self.source4_image_selector == False:\n",
    "                self.source4_zip_handle = zipfile.ZipFile(self.img_dir + zip_name, 'r')\n",
    "                self.source4_image_selector = True\n",
    "            archive = self.source4_zip_handle\n",
    "        else:\n",
    "            archive = zipfile.ZipFile(self.img_dir + zip_name, 'r')\n",
    "\n",
    "        image = archive.read(zip_dir + name)\n",
    "        image = Image.open(io.BytesIO(image))\n",
    "        #archive.close()\n",
    "        \n",
    "        return image\n",
    "\n",
    "    def get_image_from_folder(self, name):\n",
    "        \"\"\"\n",
    "        gets a image by a name gathered from file list text file\n",
    "\n",
    "        :param name: name of targeted image\n",
    "        :return: a PIL image\n",
    "        \"\"\"\n",
    "\n",
    "        image = Image.open(self.img_dir + name)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of data set using list of IDs\n",
    "\n",
    "        :return: number of samples in data set\n",
    "        \"\"\"\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Generate one item of data set.\n",
    "\n",
    "        :param index: index of item in IDs list\n",
    "\n",
    "        :return: a sample of data as a dict\n",
    "        \"\"\"\n",
    "        source = self.df.iloc[index]['Data Source']\n",
    "        image_name = self.df.iloc[index]['Image Name']\n",
    "\n",
    "        if source == 'Source-3':\n",
    "            zip_name = source + '/CheXpert-v1.0-small.zip'\n",
    "            temp = image_name.split('/')[1].split('__')\n",
    "            temp[0] = temp[0]+'-small'\n",
    "            zip_dir = '/'.join(  temp[:-1]) + '/'\n",
    "            image_name = temp[-1]\n",
    "            X = self.get_image_from_zip(source, zip_name, zip_dir, image_name)\n",
    "        \n",
    "        elif source == 'Source-4':\n",
    "            if self.train == True or self.val == True:\n",
    "                zip_dir = 'chest-xray-pneumonia/chest_xray/train/'\n",
    "            else:\n",
    "                zip_dir = 'chest-xray-pneumonia/chest_xray/test/'\n",
    "            zip_name = source + '/chest-xray-pneumonia.zip'\n",
    "            X = self.get_image_from_zip(source, zip_name, zip_dir, image_name)\n",
    "\n",
    "        elif source == 'Source-6':\n",
    "            try:\n",
    "                X = self.get_image_from_folder(source + '/' + image_name+'.jpg')\n",
    "            except FileNotFoundError:\n",
    "                X = self.get_image_from_folder(source + '/' + image_name+'.png')\n",
    "          \n",
    "          \n",
    "        else:\n",
    "            X = self.get_image_from_folder(source + '/' + image_name)\n",
    "        \n",
    "        flag = False\n",
    "        if X.mode == 'L':\n",
    "            flag = True\n",
    "        if X.mode == 'RGBA':\n",
    "            X = X.convert('RGB')\n",
    "        #X = X.convert('L')\n",
    "\n",
    "\n",
    "        # Get you label here using available pandas functions\n",
    "        y = self.df.iloc[index]['class']\n",
    "\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "            if flag:\n",
    "                X = transforms.Compose([transforms.Lambda(lambda x: x.repeat(3, 1, 1) )])(X)\n",
    "\n",
    "        if index == (self.__len__() - 1):\n",
    "            if self.source3_image_selector:  # close tarfile opened in __init__\n",
    "                self.source3_image_selector = False\n",
    "                self.source3_zip_handle.close()                \n",
    "            if self.source4_image_selector:  # close tarfile opened in __init__\n",
    "                self.source4_image_selector = False\n",
    "                self.source4_zip_handle.close()\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 370
    },
    "colab_type": "code",
    "id": "A6hUWL2y3Raq",
    "outputId": "5036b2c2-f3de-4934-f050-92f776053ea6"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 32,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 8}\n",
    "\n",
    "vparams = {'batch_size': 32,\n",
    "          'shuffle': False,\n",
    "          'num_workers': 8}\n",
    "\n",
    "max_epochs = 100\n",
    "n_splits = 5\n",
    "\n",
    "\n",
    "train_transform = transforms.Compose([transforms.Resize(size = (256, 256)),\\\n",
    "                                      transforms.RandomCrop(size = 224),\\\n",
    "                                      transforms.RandomRotation(10, resample=Image.BILINEAR), \\\n",
    "                                      transforms.ToTensor(), \\\n",
    "                                      transforms.Normalize((0.45,), (0.22,)) ])\n",
    "test_transform = transforms.Compose([ transforms.Resize(size = (256, 256)),\\\n",
    "                                      transforms.RandomCrop(size = 224),\\\n",
    "                                      transforms.ToTensor(), \\\n",
    "                                      transforms.Normalize((0.45,), (0.22,)) ])\n",
    "\n",
    "# Generators\n",
    "txt_path = '/content/drive/My Drive/Covid-19-chest-xray-classification/'\n",
    "img_dir =  '/content/drive/My Drive/Covid-19-chest-xray-classification/images/'\n",
    "\n",
    "training_set = RadiologyDataset(txt_path + 'Train_Combined.csv', img_dir, transform = train_transform, train=True, val=False, val_partition=5)\n",
    "trainLoader = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = RadiologyDataset(txt_path + 'Train_Combined.csv', img_dir, transform = test_transform, train=False, val=True, val_partition=5)\n",
    "validationLoader = data.DataLoader(validation_set, **vparams)\n",
    "\n",
    "test_set = RadiologyDataset(txt_path + 'Test_Combined.csv', img_dir, transform = test_transform, train=False, val=False, val_partition=0)\n",
    "testLoader = data.DataLoader(test_set, batch_size= 32, shuffle=False, num_workers=8)\n",
    "\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "if use_gpu:\n",
    "    print('GPU is avaialble!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PqgGhrhD4E9O"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "    # Freeze model weights\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    model.classifier[4] =  nn.Linear(in_features=4096, out_features=1000, bias=True)\n",
    "    model.classifier[5] =  nn.ReLU(inplace=True)\n",
    "    model.classifier[6] =  nn.Sequential(nn.Dropout(p=0.3, inplace=False),\n",
    "                           nn.Linear(in_features=1000, out_features=100, bias=True),\n",
    "                           nn.ReLU(inplace=True),\n",
    "                           nn.Dropout(p=0.3, inplace=False),\n",
    "                           nn.Linear(in_features=100, out_features=3, bias=True),\n",
    "                           nn.LogSoftmax(dim=1))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133,
     "referenced_widgets": [
      "9277f7206eba444284d11641dcfe9fa1",
      "e48292578b75448b99cee2bfd432dda9",
      "b04d35c7c8c24bec8c05f69b348c6968",
      "dcbe9d0a2943439a9a031680c42e2164",
      "e9bae6acf573407792d679dfdd17f947",
      "588e54e5965a4a67a2e036e72a56cf1e",
      "7e314489507943a3860fbec6d0a66d2b",
      "0fb4ab9f6ffb490aafbb28baa023e34b"
     ]
    },
    "colab_type": "code",
    "id": "0PkQK4AL5zXa",
    "outputId": "58b6bb9d-a086-46aa-dfae-4d625204d66c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/archive/v0.6.0.zip\" to /root/.cache/torch/hub/v0.6.0.zip\n",
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-4df8aa71.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9277f7206eba444284d11641dcfe9fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=244418560.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "44,419,931 total parameters.\n",
      "4,197,403 training parameters.\n"
     ]
    }
   ],
   "source": [
    "net = get_model()\n",
    "\n",
    "# Find total parameters and trainable parameters\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in net.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Vy5wRC_VNt9"
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), img_dir+'Alexnetcxr_checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHjEMCJA-FUl"
   },
   "outputs": [],
   "source": [
    "def test_accuracy_calculate():\n",
    "    correct_pred = 0\n",
    "    t.reset()\n",
    "\n",
    "    actual_labels = []\n",
    "    pred_lables = []\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for data_ in testLoader:\n",
    "            inputs,labels = data_\n",
    "            if use_gpu:\n",
    "                inputs, labels = inputs.cuda(),labels.cuda()\n",
    "            t.resume()\n",
    "            # Feedforward train data batch through model\n",
    "            output = net(inputs) \n",
    "            # Predicted class is the one with maximum probability\n",
    "            preds = torch.argmax(output,dim=1)\n",
    "            correct_pred += torch.sum(preds==labels)\n",
    "            t.pause()\n",
    "            t.step()\n",
    "\n",
    "            actual_labels = actual_labels + list(labels.cpu().detach().numpy())\n",
    "            pred_lables = pred_lables + list(preds.cpu().detach().numpy())\n",
    "\n",
    "    test_accuracy = correct_pred.item()/len(test_set)\n",
    "    print('Testing accuracy = ',test_accuracy*100, 'Computational Time = ', t.value())\n",
    "    return test_accuracy * 100, actual_labels, pred_lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RgdmtJ9C_dIJ"
   },
   "outputs": [],
   "source": [
    "### Set the Timer to compute the inference \n",
    "t = Timer(average=True)\n",
    "t.reset()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 723
    },
    "colab_type": "code",
    "id": "-FArt1jm_f-m",
    "outputId": "226b083d-82b7-484d-f0ff-c6c8d44fa438"
   },
   "outputs": [],
   "source": [
    "kfold_train_acc = []\n",
    "kfold_validation_acc = []\n",
    "kfold_test_acc = []\n",
    "\n",
    "kfold_train_loss = []\n",
    "kfold_validation_loss = []\n",
    "n_splits=5\n",
    "for sp_ in range(n_splits):\n",
    "    fold_ = sp_ + 1\n",
    "    print(\"*\"*100)\n",
    "    print(\"Training for cross-fold number:\", fold_)\n",
    "    print(\"*\"*100)\n",
    "    ##############  Initialize the Network  ###########\n",
    "\n",
    "    net = get_model()\n",
    "    if use_gpu:\n",
    "        net = net.cuda()\n",
    "        net = nn.DataParallel(net)\n",
    "\n",
    "    ##############  Define the Loss function  ##########\n",
    "    criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "    ##############  Define the Optimizer  #################\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.5)\n",
    "    #optimizer = optim.Adam(net.parameters())\n",
    "\n",
    "    ##############  Define the scheduler  (if require warm restart to set the learning rate)   #################\n",
    "    #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.1, steps_per_epoch=len(trainLoader), epochs=num_epochs, anneal_strategy='linear') #steps_per_epoch=len(trainLoader), epochs=num_epochs,\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "\n",
    "    ##############  Set the Loader -- For current k-fold  ###########\n",
    "    training_set = RadiologyDataset(txt_path + 'Train_Combined.csv', img_dir, transform = train_transform, train=True, val=False, val_partition=fold_)\n",
    "    trainLoader = data.DataLoader(training_set, **params)\n",
    "\n",
    "    validation_set = RadiologyDataset(txt_path + 'Train_Combined.csv', img_dir, transform = test_transform, train=False, val=True, val_partition=fold_)\n",
    "    validationLoader = data.DataLoader(validation_set, **vparams)\n",
    "\n",
    "\n",
    "    #############   Declare the place holder for storing -- loss and accuracy for each epoch   ########\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "\n",
    "    validation_loss =[]\n",
    "    validation_acc = []\n",
    "\n",
    "\n",
    "    num_epochs = 50\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        t.reset()  ## --- restart timer\n",
    "\n",
    "        #####################################\n",
    "        #######          Train      #########   \n",
    "        ##################################### \n",
    "        running_loss = 0.0 \n",
    "        running_corr = 0\n",
    "        for i,data_ in tqdm(enumerate(trainLoader)):\n",
    "            #print(fold_, epoch, i)\n",
    "            t.resume()   # ---- timer on\n",
    "            inputs,labels = data_\n",
    "            if use_gpu:\n",
    "                inputs, labels = inputs.cuda(),labels.cuda() \n",
    "            # Initializing model gradients to zero\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            \n",
    "            # Data feed-forward through the network\n",
    "            outputs = net(inputs)\n",
    "            # Predicted class is the one with maximum probability\n",
    "            preds = torch.argmax(outputs,dim=1)\n",
    "            # Finding the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            #scheduler.step()\n",
    "            t.pause()   # ---- timer off\n",
    "\n",
    "            # Accumulating the loss for each batch\n",
    "            running_loss += loss \n",
    "            # Accumulate number of correct predictions\n",
    "            running_corr += torch.sum(preds==labels)    \n",
    "            \n",
    "\n",
    "        epoch_loss = running_loss.item()/(i+1)   #Total loss for one epoch\n",
    "        epoch_acc = running_corr.item()/len(training_set)\n",
    "\n",
    "        train_loss.append(epoch_loss) #Saving the loss over epochs for plotting the graph\n",
    "        train_acc.append(epoch_acc) #Saving the accuracy over epochs for plotting the graph\n",
    "\n",
    "        #####################################\n",
    "        #######     Validation      #########   \n",
    "        ##################################### \n",
    "\n",
    "        val_running_loss = 0.0 \n",
    "        val_running_corr = 0\n",
    "        ### Validation \n",
    "        with torch.no_grad():   #torch.set_grad_enabled(False):\n",
    "            for j, data_ in tqdm(enumerate(validationLoader)):\n",
    "                inputs,labels = data_\n",
    "                if use_gpu:\n",
    "                    inputs, labels = inputs.cuda(),labels.cuda() \n",
    "                t.resume()   # ---- timer on\n",
    "                outputs = net(inputs)\n",
    "                preds = torch.argmax(outputs,dim=1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                t.pause()   # ---- timer off\n",
    "                val_running_loss += loss \n",
    "                val_running_corr += torch.sum(preds==labels) \n",
    "\n",
    "        val_epoch_loss = val_running_loss.item()/(j+1)   #Total loss for one epoch\n",
    "        val_epoch_acc = val_running_corr.item()/len(validation_set)\n",
    "\n",
    "        validation_loss.append(val_epoch_loss) #Saving the loss over epochs for plotting the graph\n",
    "        validation_acc.append(val_epoch_acc) #Saving the accuracy over epochs for plotting the graph    \n",
    "\n",
    "        #########   Scheduler to update the learning rate   #######\n",
    "        scheduler.step(val_epoch_loss)    \n",
    "            \n",
    "        t.step()   #-- timer average\n",
    "\n",
    "        print('Epoch {:.0f}/{:.0f} : Training loss: {:.4f} | Training Accuracy: {:.4f} : Validation loss: {:.4f} | Validation Accuracy: {:.4f} | Computational Time: {:.4f} sec'\\\n",
    "              .format(epoch+1, num_epochs, epoch_loss, epoch_acc*100, val_epoch_loss, val_epoch_acc*100, t.value()))\n",
    "        \n",
    "\n",
    "\n",
    "    ############  Calculate Test Accuracy  #######\n",
    "    test_acc, test_labels, pred_labels = test_accuracy_calculate()\n",
    "\n",
    "\n",
    "    kfold_train_acc.append(train_acc)\n",
    "    kfold_validation_acc.append(validation_acc)\n",
    "    kfold_test_acc.append(test_acc)\n",
    "\n",
    "    kfold_train_loss.append(train_loss)\n",
    "    kfold_validation_loss.append(validation_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Vd5_Jo-2vW70",
    "outputId": "a42bc0c5-ceab-4952-b477-044e38937314"
   },
   "outputs": [],
   "source": [
    "test_acc, test_labels, preds_labels = test_accuracy_calculate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "id": "4cWz_lP6krIF",
    "outputId": "308584cc-3f71-48a8-a664-62e419cb7623"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,  multilabel_confusion_matrix\n",
    "\n",
    "acc = accuracy_score(test_labels, np.round(preds_labels))*100\n",
    "cm = multilabel_confusion_matrix(test_labels, np.round(preds_labels))\n",
    "\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"#\"*20)\n",
    "    if i == 0:\n",
    "        print(\"Covid Vs Non-Covid\")\n",
    "    elif i == 1:\n",
    "        print(\"Pneumonia Vs Non-Pneumonia\")\n",
    "    else:\n",
    "        print(\"Normal Vs Non-Normal\")\n",
    "    print(\"#\"*20)\n",
    "    tn, fp, fn, tp = cm[i].ravel()\n",
    "    print('CONFUSION MATRIX --------')\n",
    "    print(cm[i])\n",
    "\n",
    "    print('\\nTEST METRICS ------------')\n",
    "    precision = tp/(tp+fp)*100\n",
    "    recall = tp/(tp+fn)*100\n",
    "    print('Accuracy: {}%'.format(acc))\n",
    "    print('Precision: {}%'.format(precision))\n",
    "    print('Recall: {}%'.format(recall))\n",
    "    print('F1-score: {}'.format(2*precision*recall/(precision+recall)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_model(train_loss, validation_loss, train_acc, validation_acc):\n",
    "    fig = plt.figure(figsize=[15,5]) \n",
    "    plt.subplot(121)\n",
    "    plt.plot(range(len(train_loss)),train_loss,'r-',label='Training Loss') \n",
    "    plt.plot(range(len(validation_loss)),validation_loss,'b-',label='Validation Loss') \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.subplot(122)\n",
    "    plt.plot(range(len(train_acc)),train_acc,'g-',label='Training Accuracy')\n",
    "    plt.plot(range(len(validation_acc)),validation_acc,'y-',label='Validation Accuracy')  \n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sp_ in range(n_splits):\n",
    "    ############  Plot - Learning Curve   ########\n",
    "    plot_train_model(kfold_train_loss[sp_], kfold_validation_loss[sp_], kfold_train_acc[sp_], kfold_validation_acc[sp_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 706
    },
    "colab_type": "code",
    "id": "ILSOtXmIt8-z",
    "outputId": "8ecf0001-23a8-4d97-c906-f011e949578f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_summary\n",
      "\n",
      "Layer_name\t\t\t\t\t\t\tNumber of Parameters\n",
      "====================================================================================================\n",
      "\n",
      "AlexNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5, inplace=False)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.5, inplace=False)\n",
      "    (4): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Sequential(\n",
      "      (0): Dropout(p=0.3, inplace=False)\n",
      "      (1): Linear(in_features=1000, out_features=100, bias=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Dropout(p=0.3, inplace=False)\n",
      "      (4): Linear(in_features=100, out_features=3, bias=True)\n",
      "      (5): LogSoftmax()\n",
      "    )\n",
      "  )\n",
      ")\t\t\t0\n",
      "====================================================================================================\n",
      "Total Params:0\n"
     ]
    }
   ],
   "source": [
    "def model_summary(model):\n",
    "    print(\"model_summary\")\n",
    "    print()\n",
    "    print(\"Layer_name\"+\"\\t\"*7+\"Number of Parameters\")\n",
    "    print(\"=\"*100)\n",
    "    model_parameters = [layer for layer in model.parameters() if layer.requires_grad]\n",
    "    layer_name = [child for child in model.children()]\n",
    "    j = 0\n",
    "    total_params = 0\n",
    "    for i in layer_name:\n",
    "        print()\n",
    "        param = 0\n",
    "        try:\n",
    "            if i.bias is not None:\n",
    "                if len(i.bias) == 0:\n",
    "                    param =model_parameters[j].numel()\n",
    "                    j = j+1\n",
    "                else:\n",
    "                    param =model_parameters[j].numel()+model_parameters[j+1].numel()\n",
    "                    j = j+2\n",
    "\n",
    "        except:\n",
    "            bias = False  \n",
    "            param = 0\n",
    "\n",
    "        print(str(i)+\"\\t\"*3+str(param))\n",
    "        total_params+=param\n",
    "    print(\"=\"*100)\n",
    "    print(f\"Total Params:{total_params}\")       \n",
    "\n",
    "model_summary(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZ7LBvTmec6z"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0fb4ab9f6ffb490aafbb28baa023e34b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "588e54e5965a4a67a2e036e72a56cf1e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e314489507943a3860fbec6d0a66d2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9277f7206eba444284d11641dcfe9fa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b04d35c7c8c24bec8c05f69b348c6968",
       "IPY_MODEL_dcbe9d0a2943439a9a031680c42e2164"
      ],
      "layout": "IPY_MODEL_e48292578b75448b99cee2bfd432dda9"
     }
    },
    "b04d35c7c8c24bec8c05f69b348c6968": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_588e54e5965a4a67a2e036e72a56cf1e",
      "max": 244418560,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e9bae6acf573407792d679dfdd17f947",
      "value": 244418560
     }
    },
    "dcbe9d0a2943439a9a031680c42e2164": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0fb4ab9f6ffb490aafbb28baa023e34b",
      "placeholder": "​",
      "style": "IPY_MODEL_7e314489507943a3860fbec6d0a66d2b",
      "value": " 233M/233M [00:01&lt;00:00, 137MB/s]"
     }
    },
    "e48292578b75448b99cee2bfd432dda9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9bae6acf573407792d679dfdd17f947": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
